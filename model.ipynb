{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haZAR\\anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ListSkip(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ListSkip , self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv1d = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=4, padding=1)\n",
    "        self.conv1dd = nn.Conv2d(in_channels=64, out_channels=512, kernel_size=3, stride=8, padding=1)\n",
    "        self.conv2d = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=4, padding=1)\n",
    "\n",
    "    def add_skip(self, skip_list):\n",
    "        el1 = skip_list[0]\n",
    "        el2 = skip_list[1]\n",
    "        el3 = skip_list[2]\n",
    "        el4 = skip_list[3]\n",
    "        # convel1_el2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        # convel2_el3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        # convel3_el4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
    "        el1_conv = self.conv1(el1)\n",
    "        el1_convd = self.conv1d(el1)\n",
    "        el1_convdd = self.conv1dd(el1)\n",
    "        el2_conv = self.conv2(el2)\n",
    "        el2_conv2d = self.conv2d(el2)\n",
    "        el3_conv = self.conv3(el3)\n",
    "\n",
    "        skip_new = [el1 , el1_conv+el2 , el1_convd+el2_conv + el3 ,el1_convdd+el2_conv2d+el3_conv+el4]\n",
    "        return skip_new\n",
    "    def forward(self,x):\n",
    "        return self.add_skip(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "    def Attention(self,q,k,v):\n",
    "        q_reshape = q.view(q.size(0),-1) #(4,2,160,64)\n",
    "        k_reshape = k.view(k.size(0),-1) #(4,2,160,64)\n",
    "        v_reshape = v.view(v.size(0),-1) #(4,2,160,64)\n",
    "        k_transposed = torch.transpose(k_reshape, 1,0) #(4,2,160,64)\n",
    "        scaled_dot_product = torch.matmul(q_reshape, k_transposed)\n",
    "        dk = torch.tensor(k_reshape.size()[-1], dtype=torch.float32) #160\n",
    "        scaled_attention_logits = scaled_dot_product / torch.sqrt(dk) #normalize\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, v_reshape)\n",
    "        output = output.view(q.size(0), q.size(1), q.size(2), q.size(3))\n",
    "\n",
    "        return attention_weights, output\n",
    "    def forward(self,q,k,v):\n",
    "        return self.Attention(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReluSIG(nn.Module):\n",
    "    def __init__(self , embedding_dim):\n",
    "        super().__init__()\n",
    "        self.gelu = nn.GELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        Relumoid = self.gelu(x)*self.sigmoid(torch.square(x))\n",
    "        return Relumoid\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.relusig = ReluSIG(out_channels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            self.relusig,\n",
    "            nn.Conv2d(out_channels,out_channels, 3, 1, 1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            self.relusig,\n",
    "            \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self , in_channels=3, out_channels=1 , features=[64,128,256,512],\n",
    "    ):\n",
    "        super(UNET , self).__init__()\n",
    "        self.ups = nn.ModuleList() #list\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2 , stride=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.att = Attention()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels,feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2 , feature , kernel_size = 2 , stride = 2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2,feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size=1)\n",
    "        self.add_skip = ListSkip()\n",
    "    def forward(self,x):\n",
    "        skip_connections = []\n",
    "        for i,down in enumerate(self.downs):\n",
    "            x = down(x)\n",
    "            \n",
    "            skip_connections.append(x) ##adding of previsous layer slip connections\n",
    "            x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        out,x = self.att(x,x,x)\n",
    "        # skip_connections_new = self.add_skip(skip_connections)\n",
    "        skip_connections_new = self.add_skip(skip_connections)\n",
    "\n",
    "\n",
    "\n",
    "        skip_connections = skip_connections_new[::-1]\n",
    "\n",
    "        for idx in range(0,len(self.ups),2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x,size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection,x),dim=1)\n",
    "\n",
    "            x= self.ups[idx+1](concat_skip)\n",
    "            \n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "        \n",
    "def load_model(model_name):\n",
    "    if model_name == 'UNet':\n",
    "        model = UNET(in_channels=1, out_channels=1)\n",
    "    else:\n",
    "        raise ValueError('Please input valid model name, {} not in model zones.'.format(model_name))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = load_model(model_name='UNet')\n",
    "    print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
